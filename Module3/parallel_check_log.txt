MAP
 
================================================================================
 Parallel Accelerator Optimizing:  Function tensor_map.<locals>._map, 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(154)  
================================================================================


Parallel loop listing for  Function tensor_map.<locals>._map, /home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py (154) 
---------------------------------------------------------------------------------------|loop #ID
    def _map(                                                                          | 
        out: Storage,                                                                  | 
        out_shape: Shape,                                                              | 
        out_strides: Strides,                                                          | 
        in_storage: Storage,                                                           | 
        in_shape: Shape,                                                               | 
        in_strides: Strides,                                                           | 
    ) -> None:                                                                         | 
        if len(in_storage) == len(out) and np.array_equal(out_strides, in_strides):    | 
            for i in prange(len(out)):-------------------------------------------------| #0
                out[i] = fn(in_storage[i])                                             | 
        else:                                                                          | 
            for idx in prange(len(out)):-----------------------------------------------| #1
                out_index = np.empty(MAX_DIMS, np.int32)                               | 
                in_index = np.empty(MAX_DIMS, np.int32)                                | 
                to_index(idx, out_shape, out_index)                                    | 
                broadcast_index(out_index, out_shape, in_shape, in_index)              | 
                                                                                       | 
                pos_in = index_to_position(in_index, in_strides)                       | 
                pos_out = index_to_position(out_index, out_strides)                    | 
                                                                                       | 
                out[pos_out] = fn(in_storage[pos_in])                                  | 
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 2 parallel for-
loop(s) (originating from loops labelled: #0, #1).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
 
---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(167) is hoisted out of the parallel loop labelled #1 (it will be performed 
before the loop is executed and reused inside the loop):
   Allocation:: out_index = np.empty(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(168) is hoisted out of the parallel loop labelled #1 (it will be performed 
before the loop is executed and reused inside the loop):
   Allocation:: in_index = np.empty(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
None
ZIP
 
================================================================================
 Parallel Accelerator Optimizing:  Function tensor_zip.<locals>._zip, 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(202)  
================================================================================


Parallel loop listing for  Function tensor_zip.<locals>._zip, /home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py (202) 
-----------------------------------------------------------------------------------|loop #ID
    def _zip(                                                                      | 
        out: Storage,                                                              | 
        out_shape: Shape,                                                          | 
        out_strides: Strides,                                                      | 
        a_storage: Storage,                                                        | 
        a_shape: Shape,                                                            | 
        a_strides: Strides,                                                        | 
        b_storage: Storage,                                                        | 
        b_shape: Shape,                                                            | 
        b_strides: Strides,                                                        | 
    ) -> None:                                                                     | 
        if (                                                                       | 
            len(a_storage) == len(b_storage)                                       | 
            and len(out) == len(a_storage)                                         | 
            and np.array_equal(out_strides, a_strides)                             | 
            and np.array_equal(a_strides, b_strides)                               | 
        ):                                                                         | 
            # considering special condition when out and a,b are stride-aligned    | 
            for i in prange(len(out)):---------------------------------------------| #2
                out[i] = fn(a_storage[i], b_storage[i])                            | 
        else:                                                                      | 
            for idx in prange(len(out)):-------------------------------------------| #3
                out_index = np.empty(MAX_DIMS, np.int32)                           | 
                a_index = np.empty(MAX_DIMS, np.int32)                             | 
                b_index = np.empty(MAX_DIMS, np.int32)                             | 
                to_index(idx, out_shape, out_index)                                | 
                                                                                   | 
                pos_out = index_to_position(out_index, out_strides)                | 
                                                                                   | 
                broadcast_index(out_index, out_shape, a_shape, a_index)            | 
                pos_a = index_to_position(a_index, a_strides)                      | 
                                                                                   | 
                broadcast_index(out_index, out_shape, b_shape, b_index)            | 
                pos_b = index_to_position(b_index, b_strides)                      | 
                                                                                   | 
                out[pos_out] = fn(a_storage[pos_a], b_storage[pos_b])              | 
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 2 parallel for-
loop(s) (originating from loops labelled: #2, #3).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
 
---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(224) is hoisted out of the parallel loop labelled #3 (it will be performed 
before the loop is executed and reused inside the loop):
   Allocation:: out_index = np.empty(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(225) is hoisted out of the parallel loop labelled #3 (it will be performed 
before the loop is executed and reused inside the loop):
   Allocation:: a_index = np.empty(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(226) is hoisted out of the parallel loop labelled #3 (it will be performed 
before the loop is executed and reused inside the loop):
   Allocation:: b_index = np.empty(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
None
REDUCE
 
================================================================================
 Parallel Accelerator Optimizing:  Function tensor_reduce.<locals>._reduce, 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(261)  
================================================================================


Parallel loop listing for  Function tensor_reduce.<locals>._reduce, /home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py (261) 
-----------------------------------------------------------------|loop #ID
    def _reduce(                                                 | 
        out: Storage,                                            | 
        out_shape: Shape,                                        | 
        out_strides: Strides,                                    | 
        a_storage: Storage,                                      | 
        a_shape: Shape,                                          | 
        a_strides: Strides,                                      | 
        reduce_dim: int,                                         | 
    ) -> None:                                                   | 
        for out_i in prange(len(out)):---------------------------| #4
            out_idx = np.empty(len(a_shape), dtype=np.int32)     | 
            to_index(out_i, out_shape, out_idx)                  | 
            pos_out = index_to_position(out_idx, out_strides)    | 
            pos_a = index_to_position(out_idx, a_strides)        | 
                                                                 | 
            acc = out[pos_out]                                   | 
            for _ in range(a_shape[reduce_dim]):                 | 
                acc = fn(acc, a_storage[pos_a])                  | 
                pos_a += a_strides[reduce_dim]                   | 
                                                                 | 
            out[pos_out] = acc                                   | 
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #4).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
 
---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(271) is hoisted out of the parallel loop labelled #4 (it will be performed 
before the loop is executed and reused inside the loop):
   Allocation:: out_idx = np.empty(len(a_shape), dtype=np.int32)
    - numpy.empty() is used for the allocation.
None
MATRIX MULTIPLY
 
================================================================================
 Parallel Accelerator Optimizing:  Function _tensor_matrix_multiply, 
/home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py 
(286)  
================================================================================


Parallel loop listing for  Function _tensor_matrix_multiply, /home/fabio/Documents/Cornell/MLE/mle-module-3-Fabio752/minitorch/fast_ops.py (286) 
-------------------------------------------------------------------------------|loop #ID
def _tensor_matrix_multiply(                                                   | 
    out: Storage,                                                              | 
    out_shape: Shape,                                                          | 
    out_strides: Strides,                                                      | 
    a_storage: Storage,                                                        | 
    a_shape: Shape,                                                            | 
    a_strides: Strides,                                                        | 
    b_storage: Storage,                                                        | 
    b_shape: Shape,                                                            | 
    b_strides: Strides,                                                        | 
) -> None:                                                                     | 
    """                                                                        | 
    NUMBA tensor matrix multiply function.                                     | 
                                                                               | 
    Should work for any tensor shapes that broadcast as long as                | 
                                                                               | 
    ```                                                                        | 
    assert a_shape[-1] == b_shape[-2]                                          | 
    ```                                                                        | 
                                                                               | 
    Optimizations:                                                             | 
                                                                               | 
    * Outer loop in parallel                                                   | 
    * No index buffers or function calls                                       | 
    * Inner loop should have no global writes, 1 multiply.                     | 
                                                                               | 
                                                                               | 
    Args:                                                                      | 
        out (Storage): storage for `out` tensor                                | 
        out_shape (Shape): shape for `out` tensor                              | 
        out_strides (Strides): strides for `out` tensor                        | 
        a_storage (Storage): storage for `a` tensor                            | 
        a_shape (Shape): shape for `a` tensor                                  | 
        a_strides (Strides): strides for `a` tensor                            | 
        b_storage (Storage): storage for `b` tensor                            | 
        b_shape (Shape): shape for `b` tensor                                  | 
        b_strides (Strides): strides for `b` tensor                            | 
                                                                               | 
    Returns:                                                                   | 
        None : Fills in `out`                                                  | 
    """                                                                        | 
    a_batch_stride = a_strides[0] if a_shape[0] > 1 else 0                     | 
    b_batch_stride = b_strides[0] if b_shape[0] > 1 else 0                     | 
                                                                               | 
    assert a_shape[-1] == b_shape[-2]                                          | 
                                                                               | 
    for pos in prange(len(out)):-----------------------------------------------| #5
        out_idx0 = pos // (out_shape[-1] * out_shape[-2])                      | 
        out_idx1 = (pos % (out_shape[-1] * out_shape[-2])) // out_shape[-1]    | 
        out_idx2 = pos % out_shape[-1]                                         | 
                                                                               | 
        out_pos0 = out_idx0 * out_strides[0]                                   | 
        out_pos1 = out_idx1 * out_strides[1]                                   | 
        out_pos2 = out_idx2 * out_strides[2]                                   | 
        out_position = out_pos0 + out_pos1 + out_pos2                          | 
                                                                               | 
        a_start = out_idx0 * a_batch_stride + out_idx1 * a_strides[1]          | 
        b_start = out_idx0 * b_batch_stride + out_idx2 * b_strides[2]          | 
                                                                               | 
        acc = 0                                                                | 
        for i in range(a_shape[-1]):                                           | 
            acc += (                                                           | 
                a_storage[a_start + i * a_strides[2]]                          | 
                * b_storage[b_start + i * b_strides[1]]                        | 
            )                                                                  | 
        out[out_position] = acc                                                | 
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #5).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
 
---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found
None
